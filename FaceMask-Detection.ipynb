{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from shutil import copyfile\n",
    "from os import getcwd\n",
    "from os import listdir\n",
    "import cv2\n",
    "from tensorflow.keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "import imutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image  as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The number of images with facemask labelled 'yes': 690\nThe number of images with facemask labelled 'no': 686\n"
    }
   ],
   "source": [
    "print(\"The number of images with facemask labelled 'yes':\",len(os.listdir('facemask-dataset/data/with_mask')))\n",
    "print(\"The number of images with facemask labelled 'no':\",len(os.listdir('facemask-dataset/data/without_mask')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of examples: 1650\nPercentage of positive examples: 50.06060606060606%, number of pos examples: 826\nPercentage of negative examples: 49.93939393939394%, number of neg examples: 824\n"
    }
   ],
   "source": [
    "def data_summary(main_path):\n",
    "    \n",
    "    yes_path = main_path+'yesreal'\n",
    "    no_path = main_path+'noreal'\n",
    "        \n",
    "    # number of files (images) that are in the the folder named 'yes' that represent tumorous (positive) examples\n",
    "    m_pos = len(listdir(yes_path))\n",
    "    # number of files (images) that are in the the folder named 'no' that represent non-tumorous (negative) examples\n",
    "    m_neg = len(listdir(no_path))\n",
    "    # number of all examples\n",
    "    m = (m_pos+m_neg)\n",
    "    \n",
    "    pos_prec = (m_pos* 100.0)/ m\n",
    "    neg_prec = (m_neg* 100.0)/ m\n",
    "    \n",
    "    print(f\"Number of examples: {m}\")\n",
    "    print(f\"Percentage of positive examples: {pos_prec}%, number of pos examples: {m_pos}\") \n",
    "    print(f\"Percentage of negative examples: {neg_prec}%, number of neg examples: {m_neg}\") \n",
    "    \n",
    "augmented_data_path = 'facemask-dataset/trial1/augmenteddata1/'    \n",
    "data_summary(augmented_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
    "    dataset = []\n",
    "    \n",
    "    for unitData in os.listdir(SOURCE):\n",
    "        data = SOURCE + unitData\n",
    "        if(os.path.getsize(data) > 0):\n",
    "            dataset.append(unitData)\n",
    "        else:\n",
    "            print('Skipped ' + unitData)\n",
    "            print('Invalid file i.e zero size')\n",
    "    \n",
    "    train_set_length = int(len(dataset) * SPLIT_SIZE)\n",
    "    test_set_length = int(len(dataset) - train_set_length)\n",
    "    shuffled_set = random.sample(dataset, len(dataset))\n",
    "    train_set = dataset[0:train_set_length]\n",
    "    test_set = dataset[-test_set_length:]\n",
    "       \n",
    "    for unitData in train_set:\n",
    "        temp_train_set = SOURCE + unitData\n",
    "        final_train_set = TRAINING + unitData\n",
    "        copyfile(temp_train_set, final_train_set)\n",
    "    \n",
    "    for unitData in test_set:\n",
    "        temp_test_set = SOURCE + unitData\n",
    "        final_test_set = TESTING + unitData\n",
    "        copyfile(temp_test_set, final_test_set)\n",
    "        \n",
    "        \n",
    "YES_SOURCE_DIR = \"facemask-dataset/trial1/augmenteddata1/yesreal/\"\n",
    "TRAINING_YES_DIR = \"facemask-dataset/trial1/augmenteddata1/training/yes1/\"\n",
    "TESTING_YES_DIR = \"facemask-dataset/trial1/augmenteddata1/testing/yes1/\"\n",
    "NO_SOURCE_DIR = \"facemask-dataset/trial1/augmenteddata1/noreal/\"\n",
    "TRAINING_NO_DIR = \"facemask-dataset/trial1/augmenteddata1/training/no1/\"\n",
    "TESTING_NO_DIR = \"facemask-dataset/trial1/augmenteddata1/testing/no1/\"\n",
    "split_size = .8\n",
    "split_data(YES_SOURCE_DIR, TRAINING_YES_DIR, TESTING_YES_DIR, split_size)\n",
    "split_data(NO_SOURCE_DIR, TRAINING_NO_DIR, TESTING_NO_DIR, split_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The number of images with facemask in the training set labelled 'yes': 660\nThe number of images with facemask in the test set labelled 'yes': 166\nThe number of images without facemask in the training set labelled 'no': 659\nThe number of images without facemask in the test set labelled 'no': 165\n"
    }
   ],
   "source": [
    "print(\"The number of images with facemask in the training set labelled 'yes':\", len(os.listdir('facemask-dataset/trial1/augmenteddata1/training/yes1')))\n",
    "print(\"The number of images with facemask in the test set labelled 'yes':\", len(os.listdir('facemask-dataset/trial1/augmenteddata1/testing/yes1')))\n",
    "print(\"The number of images without facemask in the training set labelled 'no':\", len(os.listdir('facemask-dataset/trial1/augmenteddata1/training/no1')))\n",
    "print(\"The number of images without facemask in the test set labelled 'no':\", len(os.listdir('facemask-dataset/trial1/augmenteddata1/testing/no1')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(100, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(100, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 1319 images belonging to 2 classes.\nFound 331 images belonging to 2 classes.\n"
    }
   ],
   "source": [
    "TRAINING_DIR = \"facemask-dataset/trial1/augmenteddata1/training\"\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(TRAINING_DIR, \n",
    "                                                    batch_size=10, \n",
    "                                                    target_size=(150, 150))\n",
    "VALIDATION_DIR = \"facemask-dataset/trial1/augmenteddata1/testing\"\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR, \n",
    "                                                         batch_size=10, \n",
    "                                                         target_size=(150, 150))\n",
    "checkpoint = ModelCheckpoint('model-{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From <ipython-input-8-fb50dbaa623d>:4: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use Model.fit, which supports generators.\nWARNING:tensorflow:sample_weight modes were coerced from\n  ...\n    to  \n  ['...']\nWARNING:tensorflow:sample_weight modes were coerced from\n  ...\n    to  \n  ['...']\nTrain for 132 steps, validate for 34 steps\nEpoch 1/30\n131/132 [============================>.] - ETA: 0s - loss: 0.3974 - acc: 0.8266WARNING:tensorflow:From C:\\python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nINFO:tensorflow:Assets written to: model-001.model\\assets\n132/132 [==============================] - 92s 698ms/step - loss: 0.3963 - acc: 0.8271 - val_loss: 0.2122 - val_acc: 0.9094\nEpoch 2/30\n132/132 [==============================] - 82s 623ms/step - loss: 0.2591 - acc: 0.9030 - val_loss: 0.4546 - val_acc: 0.8248\nEpoch 3/30\n131/132 [============================>.] - ETA: 0s - loss: 0.2139 - acc: 0.9274INFO:tensorflow:Assets written to: model-003.model\\assets\n132/132 [==============================] - 91s 693ms/step - loss: 0.2158 - acc: 0.9257 - val_loss: 0.1548 - val_acc: 0.9517\nEpoch 4/30\n132/132 [==============================] - 92s 696ms/step - loss: 0.1753 - acc: 0.9447 - val_loss: 0.1930 - val_acc: 0.9366\nEpoch 5/30\n132/132 [==============================] - 93s 703ms/step - loss: 0.2507 - acc: 0.9045 - val_loss: 0.2520 - val_acc: 0.9063\nEpoch 6/30\n132/132 [==============================] - 93s 707ms/step - loss: 0.2653 - acc: 0.9022 - val_loss: 0.1955 - val_acc: 0.9033\nEpoch 7/30\n131/132 [============================>.] - ETA: 0s - loss: 0.1856 - acc: 0.9381INFO:tensorflow:Assets written to: model-007.model\\assets\n132/132 [==============================] - 94s 715ms/step - loss: 0.1843 - acc: 0.9386 - val_loss: 0.1337 - val_acc: 0.9426\nEpoch 8/30\n132/132 [==============================] - 93s 705ms/step - loss: 0.1829 - acc: 0.9340 - val_loss: 0.1474 - val_acc: 0.9456\nEpoch 9/30\n131/132 [============================>.] - ETA: 0s - loss: 0.2106 - acc: 0.9312INFO:tensorflow:Assets written to: model-009.model\\assets\n132/132 [==============================] - 95s 717ms/step - loss: 0.2093 - acc: 0.9318 - val_loss: 0.1251 - val_acc: 0.9456\nEpoch 10/30\n132/132 [==============================] - 96s 729ms/step - loss: 0.1743 - acc: 0.9393 - val_loss: 0.1667 - val_acc: 0.9366\nEpoch 11/30\n132/132 [==============================] - 94s 711ms/step - loss: 0.1498 - acc: 0.9454 - val_loss: 0.4532 - val_acc: 0.8671\nEpoch 12/30\n132/132 [==============================] - 96s 730ms/step - loss: 0.1970 - acc: 0.9249 - val_loss: 0.2168 - val_acc: 0.9154\nEpoch 13/30\n132/132 [==============================] - 96s 730ms/step - loss: 0.1107 - acc: 0.9538 - val_loss: 0.1259 - val_acc: 0.9426\nEpoch 14/30\n132/132 [==============================] - 94s 714ms/step - loss: 0.0974 - acc: 0.9613 - val_loss: 0.2415 - val_acc: 0.9154\nEpoch 15/30\n132/132 [==============================] - 94s 709ms/step - loss: 0.1796 - acc: 0.9287 - val_loss: 0.2192 - val_acc: 0.8943\nEpoch 16/30\n132/132 [==============================] - 95s 720ms/step - loss: 0.1377 - acc: 0.9500 - val_loss: 0.2534 - val_acc: 0.8973\nEpoch 17/30\n132/132 [==============================] - 95s 720ms/step - loss: 0.1635 - acc: 0.9553 - val_loss: 0.1322 - val_acc: 0.9607\nEpoch 18/30\n131/132 [============================>.] - ETA: 0s - loss: 0.1222 - acc: 0.9580INFO:tensorflow:Assets written to: model-018.model\\assets\n132/132 [==============================] - 98s 741ms/step - loss: 0.1215 - acc: 0.9583 - val_loss: 0.1216 - val_acc: 0.9607\nEpoch 19/30\n131/132 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9656INFO:tensorflow:Assets written to: model-019.model\\assets\n132/132 [==============================] - 95s 721ms/step - loss: 0.1029 - acc: 0.9659 - val_loss: 0.1195 - val_acc: 0.9517\nEpoch 20/30\n131/132 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.9694INFO:tensorflow:Assets written to: model-020.model\\assets\n132/132 [==============================] - 98s 746ms/step - loss: 0.0967 - acc: 0.9689 - val_loss: 0.1111 - val_acc: 0.9456\nEpoch 21/30\n132/132 [==============================] - 93s 706ms/step - loss: 0.1339 - acc: 0.9500 - val_loss: 0.1204 - val_acc: 0.9456\nEpoch 22/30\n132/132 [==============================] - 91s 693ms/step - loss: 0.1174 - acc: 0.9575 - val_loss: 0.1547 - val_acc: 0.9245\nEpoch 23/30\n132/132 [==============================] - 95s 721ms/step - loss: 0.1186 - acc: 0.9644 - val_loss: 0.1211 - val_acc: 0.9547\nEpoch 24/30\n132/132 [==============================] - 95s 719ms/step - loss: 0.0999 - acc: 0.9659 - val_loss: 0.1303 - val_acc: 0.9396\nEpoch 25/30\n132/132 [==============================] - 89s 678ms/step - loss: 0.0920 - acc: 0.9666 - val_loss: 0.1368 - val_acc: 0.9486\nEpoch 26/30\n132/132 [==============================] - 92s 695ms/step - loss: 0.1037 - acc: 0.9575 - val_loss: 0.1931 - val_acc: 0.9215\nEpoch 27/30\n132/132 [==============================] - 92s 698ms/step - loss: 0.2844 - acc: 0.8992 - val_loss: 0.1528 - val_acc: 0.9275\nEpoch 28/30\n132/132 [==============================] - 91s 690ms/step - loss: 0.1247 - acc: 0.9598 - val_loss: 0.1669 - val_acc: 0.9154\nEpoch 29/30\n132/132 [==============================] - 93s 708ms/step - loss: 0.1248 - acc: 0.9606 - val_loss: 0.1492 - val_acc: 0.9245\nEpoch 30/30\n132/132 [==============================] - 91s 688ms/step - loss: 0.1143 - acc: 0.9560 - val_loss: 0.1265 - val_acc: 0.9486\n"
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                              epochs=30,\n",
    "                              validation_data=validation_generator,\n",
    "                              callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_clsfr=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-b9feb4896df5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# Resize the image to speed up detection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mmini\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# detect MultiScale / faces\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "labels_dict={0:'with_mask',1:'without_mask'}\n",
    "color_dict={1:(0,0,255),0:(0,255,0)}\n",
    "\n",
    "size = 4\n",
    "webcam = cv2.VideoCapture(0) #Use camera 0\n",
    "address = \"https://192.168.43.1:8080/video\"\n",
    "webcam.open(address)\n",
    "\n",
    "# We load the xml file\n",
    "classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "while True:\n",
    "    (rval, im) = webcam.read()\n",
    "    im=cv2.flip(im,1,1) #Flip to act as a mirror\n",
    "\n",
    "    # Resize the image to speed up detection\n",
    "    mini = cv2.resize(im, (im.shape[1] // size, im.shape[0] // size))\n",
    "\n",
    "    # detect MultiScale / faces \n",
    "    faces = classifier.detectMultiScale(mini)\n",
    "\n",
    "    # Draw rectangles around each face\n",
    "    for f in faces:\n",
    "        (x, y, w, h) = [v * size for v in f] #Scale the shapesize backup\n",
    "        #Save just the rectangle faces in SubRecFaces\n",
    "        face_img = im[y:y+h, x:x+w]\n",
    "        resized=cv2.resize(face_img,(150,150))\n",
    "        normalized=resized/255.0\n",
    "        reshaped=np.reshape(normalized,(1,150,150,3))\n",
    "        reshaped = np.vstack([reshaped])\n",
    "        result=model.predict(reshaped)\n",
    "        #print(result)\n",
    "        \n",
    "        label=np.argmax(result,axis=1)[0]\n",
    "      \n",
    "        cv2.rectangle(im,(x,y),(x+w,y+h),color_dict[label],2)\n",
    "        cv2.rectangle(im,(x,y-40),(x+w,y),color_dict[label],-1)\n",
    "        cv2.putText(im, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "        \n",
    "    # Show the image\n",
    "    cv2.imshow('LIVE', im)\n",
    "    # if Esc key is press then break out of the loop \n",
    "    if cv2.waitKey(10) & 0xFF == ord(\"q\"): #The Esc key\n",
    "        break\n",
    "# Stop video\n",
    "webcam.release()\n",
    "\n",
    "# Close all started windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python_defaultSpec_1595325239395"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}